# -*- coding: utf-8 -*-
"""NER_BERT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17CYqOVYLgAnZL5t7rZMSlbWjjT4x_7ML
"""

import numpy as np
import torch
from torch import nn
import nltk
nltk.download('stopwords')
import pandas as pd
from nltk.corpus import stopwords
import torch.optim as optim
from tqdm import tqdm
import os
import pandas as pd
import torch
from transformers import BertTokenizer
from transformers import BertModel
bert_model=BertModel.from_pretrained('/home/svu/e0401988/NLP/classification')
tokenizer = BertTokenizer.from_pretrained('/home/svu/e0401988/NLP/classification')

# contractions = { 
# "n't": "not",
# "'ve": "have",
# "'d": "would",
# "'ll": "will",
# "'s": "is",
# "'m": "am",
# "ma'am": "madam",
# "'re": "they are"
# }

# def clean_text(text, remove_stopwords = False):
#     '''Remove unwanted characters, stopwords, and format the text to create fewer nulls word embeddings'''
    
#     # Convert words to lower case
#     text = text.lower()
    
#     # Replace contractions with their longer forms 
#     if True:
#         text = text.split()
#         new_text = []
#         for word in text:
#             if word in contractions:
#                 new_text.append(contractions[word])
#             else:
#                 new_text.append(word)
#         text = " ".join(new_text)
    
#     # Format words and remove unwanted characters
#     text = re.sub(r'https?:\/\/.*[\r\n]*', '', text, flags=re.MULTILINE)
#     text = re.sub(r'\<a href', ' ', text)
#     text = re.sub(r'&amp;', '', text) 
#     text = re.sub(r'[_"\-;%()|+&=*%.,!?:#$@\[\]/]', ' ', text)
#     text = re.sub(r'<br />', ' ', text)
#     text = re.sub(r'\'', ' ', text)
    
#     # Optionally, remove stop words
#     if remove_stopwords:
#         text = text.split()
#         stops = set(stopwords.words("english"))
#         text = [w for w in text if not w in stops]
#         text = " ".join(text)

#     return text

# embeddings_index = {}
# with open('/content/drive/My Drive/numberbatch-en.txt', encoding='utf-8') as f:
#     for line in f:
#         values = line.split(' ')
#         word = values[0]
#         embedding = np.asarray(values[1:], dtype='float32')
#         embeddings_index[word] = embedding

training_data=[]
with open('/home/svu/e0401988/NLP/NER/train.txt','r') as f:
  line=f.readline()
  while(line):
    line=f.readline()
    if(line.strip()==''):
       training_data.append({'sent':'','label':''})
    else: 
      line=line.split()  
      training_data[-1]['sent']+=' '+str(line[0])
      training_data[-1]['label']+=' '+str(line[-1])

validation_data=[]
with open('/home/svu/e0401988/NLP/NER/valid.txt','r') as f:
  line=f.readline()
  while(line):
    line=f.readline()
    if(line.strip()==''):
       validation_data.append({'sent':'','label':''})
    else: 
      line=line.split()  
      validation_data[-1]['sent']+=' '+str(line[0])
      validation_data[-1]['label']+=' '+str(line[-1])

df=pd.DataFrame(training_data)
df_valid=pd.DataFrame(validation_data)

df['label'].replace('',np.nan,inplace=True)
df_valid['label'].replace('',np.nan,inplace=True)

print(df.isna().sum())
print(df_valid.isna().sum())

df.dropna(inplace=True)
df_valid.dropna(inplace=True)

# def count_words(count_dict, text):
#     '''Count the number of occurrences of each word in a set of text'''
#     for sentence in text:
#         for word in sentence.split():
#             if word not in count_dict:
#                 count_dict[word] = 1
#             else:
#                 count_dict[word] += 1

# word_counts = {}

# count_words(word_counts, df['sent'].values)

# missing_words = 0
# threshold = 20

# for word, count in word_counts.items():
#     if count > threshold:
#         if word not in embeddings_index:
#             missing_words += 1
            
# missing_ratio = round(missing_words/len(word_counts),4)*100
            
# print("Number of words missing from CN:", missing_words)
# print("Percent of words that are missing from vocabulary: {}%".format(missing_ratio))


def convert_to_ints(sent):
    tokens=tokenizer.tokenize(sent)
    tokens=tokenizer.convert_tokens_to_ids(tokens)     
    return tokens

df['encoding']=df['sent'].apply(lambda x:convert_to_ints(x))
df['attn_mask']=df['encoding'].apply(lambda x:[0 if tok==tokenizer.convert_tokens_to_ids('[PAD]') else 1 for tok in x])
df_valid['encoding']=df_valid['sent'].apply(lambda x:convert_to_ints(x))
df_valid['attn_mask']=df_valid['encoding'].apply(lambda x:[0 if tok==tokenizer.convert_tokens_to_ids('[PAD]') else 1 for tok in x])

unique_labels=np.unique(''.join(list(df['label'])).split())
tag_to_index={}
index_to_tag={}
for i,lbl in enumerate(list(unique_labels)):
  tag_to_index[lbl]=i
  index_to_tag[i]=lbl

def argmax(vec):
    # return the argmax as a python int
    _, idx = torch.max(vec, 1)
    return idx.item()
def log_sum_exp(vec):
    max_score = vec[0, argmax(vec)]
    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])
    return max_score + \
        torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))

class BERT_CRF(nn.Module):
    def __init__(self,hidden_size,tag_to_index):
        super(BERT_CRF, self).__init__()
        self.bert_layer = bert_model
        # for p in self.bert_layer.parameters():
        #     p.requires_grad = False

        self.target_size=len(tag_to_index)
        # self.embed = nn.Embedding(vocab_size, embedding_size)
        # self.lstm = nn.LSTM(embedding_size, hidden_size // 2,num_layers=2, bidirectional=True)
        self.hidden2tag = nn.Linear(hidden_size, self.target_size)
        self.transitions = nn.Parameter(torch.randn(self.target_size, self.target_size))
        self.start_transition=nn.Parameter(torch.randn(1, self.target_size))
        self.end_transition=nn.Parameter(torch.randn(1, self.target_size))
        # self.hidden=self.init_hidden(hidden_size)
    def _get_lstm_features(self,sentence,attn_mask):
        cont_reps, _ = self.bert_layer(sentence, attention_mask = attn_mask)
        output=self.hidden2tag(cont_reps)
        return output
    def _forward(self,features):   
        temp= self.start_transition.data+features[0,0,:]
        labels=[]
        lbl=torch.argmax(temp,1)
        # score=torch.max(veterbi_var)
        labels.append(lbl)
        # labels.append(argmax(veterbi_var))
        # score=torch.max(veterbi_var)        
        for indx in range(1,features.shape[1]):
            veterbi_var=[]
            for tag in range(features.shape[2]):                             
                veterbi_var.append(log_sum_exp(temp+self.transitions[:,tag].view(1,9)+features[0,indx,tag].expand(1,9)).view(1,1))
            # print(veterbi_var)                   
            temp=torch.cat(veterbi_var,dim=1)
            # print(temp.shape)
            lbl=torch.argmax(temp,1)
                # score=torch.max(veterbi_var)
            labels.append(lbl)
            # score=torch.max(veterbi_var)   
        temp+=self.end_transition.view(1,-1)
        # print(torch.min(temp))
        score=log_sum_exp(temp)
        return score,labels
    def _score_sentence(self,feats,tags):
        score=feats[0,0,tags[0]]+self.start_transition[0,tags[0]]
        for indx in range(1,feats.shape[1]):
            # print(feats.shape,tags[indx],indx,tags.shape)
            score+=feats[0,indx,tags[indx]]+self.transitions[tags[indx-1],tags[indx]]
        score+=self.end_transition[0,tags[-1]] 
        return score
    def _neg_log_likelihood(self,sentence,targets,attn_mask):
        feats = self._get_lstm_features(sentence,attn_mask)
        forward_score,_=self._forward(feats)        
        gold_score = self._score_sentence(feats, targets.view(-1)) 
        # print('forward score {}'.format(forward_score),'gold score {}'.format(gold_score)) 
        loss=forward_score - gold_score     
        return loss
    def _viterbi(self,features):
        # print(self.start_transition.data,features)
        temp= self.start_transition.data.view(1,-1)+features[0,0,:].view(1,-1)
        labels=[]
        # print(temp)
        temp,lbl=torch.max(temp,1)
        # print(temp,lbl)
        labels.append(lbl)     
        for indx in range(1,features.shape[0]):
            veterbi_var=[]
            for tag in range(features.shape[2]):                            
                veterbi_var.append(temp.view(1,1)+self.transitions[lbl[-1],tag].view(1)+features[indx,0,tag].view(1))
            # print(veterbi_var)                   
            temp=torch.cat(veterbi_var,dim=1)
            # lbl=torch.argmax(temp,1)
            temp,lbl=torch.max(temp,1)
            labels.append(lbl)  
        temp=temp.view(1,1)+self.end_transition[0,lbl].view(1,1)
        return temp,labels  
    def _predict(self,sentence):
        feats = self._get_lstm_features(sentence)
        score,labels=self._viterbi(feats)
        return labels

df['label_tokenized']=df['label'].apply(lambda x: x.split())
df_valid['label_tokenized']=df_valid['label'].apply(lambda x: x.split())

for index,row in df.iterrows():
  tokenized=tokenizer.tokenize(row['sent'])
  if(len(row['encoding'])!=len(row['label_tokenized']) and sum([1 if(tok.startswith('##')) else 0 for tok in tokenized])>0):
    for i in range(len(tokenized)):
      if(tokenized[i].startswith('##')):
        df.iloc[index]['label_tokenized'].insert(i,'O')

for index,row in df_valid.iterrows():
  tokenized=tokenizer.tokenize(row['sent'])
  if(len(row['encoding'])!=len(row['label_tokenized']) and sum([1 if(tok.startswith('##')) else 0 for tok in tokenized])>0):
    for i in range(len(tokenized)):
      if(tokenized[i].startswith('##')):
        df.iloc[index]['label_tokenized'].insert(i,'O')

mask=[]
for index,row in df.iterrows():
  mask.append(len(row['encoding'])==len(row['label_tokenized']))
df=df[mask]

mask=[]
for index,row in df_valid.iterrows():
  mask.append(len(row['encoding'])==len(row['label_tokenized']))
df_valid=df_valid[mask]

count=0
for index,row in df.iterrows():
  if not(len(row['label_tokenized'])==len(row['encoding'])):
    count+=1  
print(count)

import pickle
from torch.nn.utils import clip_grad_norm_
# vocab_size=len(vocab_to_int)
hidden_size=768
# embedding_size=50
df_valid.dropna(inplace=True)
device = 'cuda' if torch.cuda.is_available() else 'cpu'
model=BERT_CRF(hidden_size,tag_to_index).to(device)
if(os.path.exists('/home/svu/e0401988/NLP/NER/model_BERT.pt')):
  model.load_state_dict(torch.load('/home/svu/e0401988/NLP/NER/model_BERT.pt'))
optimizer = optim.SGD(model.parameters(), lr=1e-5, weight_decay=1e-4)
losses=[]
if(os.path.exists('/home/svu/e0401988/NLP/NER/losses_BERT.pkl')):
  with open('/home/svu/e0401988/NLP/NER/losses_BERT.pkl','rb') as f:
    losses=pickle.load(f)
for epoch in tqdm(range(50)): 
    for index,row in df.iterrows():
        model.zero_grad()
        sent=torch.tensor(row['encoding'],dtype=torch.long).view(1,-1).to(device)
        targets = torch.tensor([tag_to_index[t] for t in row['label_tokenized']], dtype=torch.long).view(1,-1).to(device)
        attn_mask=torch.tensor(row['attn_mask'],dtype=torch.long).view(1,-1).to(device)
        loss = model._neg_log_likelihood(sent, targets,attn_mask)
        loss.backward()
        # print(loss.data.item())
        clip_grad_norm_(model.parameters(), 1)
        optimizer.step() 
    val_loss=0    
    for index,row in df_valid.iterrows():
        model.zero_grad()
        sent=torch.tensor(row['encoding'],dtype=torch.long).view(1,-1).to(device)
        targets = torch.tensor([tag_to_index[t] for t in row['label_tokenized']], dtype=torch.long).view(1,-1).to(device)
        attn_mask=torch.tensor(row['attn_mask'],dtype=torch.long).view(1,-1).to(device)
        val_loss+= model._neg_log_likelihood(sent, targets, attn_mask).data.item()    
    val_loss=val_loss/len(df_valid)
    print('training loss:{}'.format(loss),'validation loss:{}'.format(val_loss))
    if(len(losses)>0 and val_loss<min(losses)):
      torch.save(model.state_dict(), '/home/svu/e0401988/NLP/NER/model_BERT.pt')            
    losses.append(val_loss)

with open('/home/svu/e0401988/NLP/NER/losses_BERT.pkl','wb') as f:
  pickle.dump(losses,f) 

